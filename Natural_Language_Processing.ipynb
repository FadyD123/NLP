{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMz/FMkm6YRdzOUb8sHTXq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FadyD123/NLP/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzGrdPXqUDHI"
      },
      "source": [
        "#nltk is the library used for NLP \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTMakWD4ps0H"
      },
      "source": [
        "import nltk\n",
        "# nltk library is the library used for NLP "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsknNqlbVCJn"
      },
      "source": [
        "\n",
        "\n",
        "1.   **sent_tokenize** is used to split the text into sentences\n",
        "\n",
        "2.   **word_tokenize** is used to split the text into words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlHJ53SDtxvs",
        "outputId": "8d586f3a-d5ed-4e59-c421-c58a47166697"
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "sample_text = \"Hello there, how are you? The weather is so nice today.\"\n",
        "\n",
        "tokenized_sentences = sent_tokenize(sample_text)\n",
        "print(tokenized_sentences)\n",
        "\n",
        "tokenized_words = word_tokenize(sample_text)\n",
        "print(tokenized_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello there, how are you?', 'The weather is so nice today.']\n",
            "['Hello', 'there', ',', 'how', 'are', 'you', '?', 'The', 'weather', 'is', 'so', 'nice', 'today', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ioaCvbCVth-"
      },
      "source": [
        "**here , we have another way to split a text into words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKXOZnpISygP",
        "outputId": "d14c9f2a-cb8a-4ec8-d8f2-3e556aabd4e3"
      },
      "source": [
        "sample_text = \"Hello there Mr. Brown, how are you? The weather is so nice today. I like machine learning.\"\n",
        "\n",
        "tokenized_sentences = sent_tokenize(sample_text)\n",
        "\n",
        "for sentence in tokenized_sentences:\n",
        "  print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello there Mr. Brown, how are you?\n",
            "The weather is so nice today.\n",
            "I like machine learning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpdu8-hiXX9l"
      },
      "source": [
        "# Normalization is the process that converts a list of words to a more uniform sequence. This is useful for preparing text for processing in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOjjwfynaq2D"
      },
      "source": [
        "**The first method is called PorterStemmer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1DyPhhVXWR3",
        "outputId": "4ce9cf56-ccd0-4765-d912-24390fef69a2"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem(\"illegal\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "illeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwmc6IVXqHk",
        "outputId": "db0693f5-2ccd-4677-b847-2a082dfc7455"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "sample_words = [\"legal\", \"illegal\", \"legally\", \"leganize\"]\n",
        "for word in sample_words:\n",
        "  print(ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "legal\n",
            "illeg\n",
            "legal\n",
            "legan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XwODCzUaMed"
      },
      "source": [
        "**The second method called WordNetLemmatizer is more efficient**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEAlvmrNZ2fe",
        "outputId": "b5e06874-6f37-486f-dffa-2796172cff5d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"puppies\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"monkeys\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"dices\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"phones\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"better\"))\n",
        "# You need to precise the type of the word \"better\"\n",
        "print(lemmatizer.lemmatize(\"better\", pos = 'a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "puppy\n",
            "monkey\n",
            "dice\n",
            "phone\n",
            "better\n",
            "good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2hYo3ded4Br"
      },
      "source": [
        "# Part of speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZanCL2W4c62i",
        "outputId": "72f0e710-c329-4bf0-c0ce-0184e8ba2558"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "sample_text = \"I enjoy when I code machine learning algorithms using python language\"\n",
        "\n",
        "tokenized_words = word_tokenize(sample_text)\n",
        "\n",
        "tag = pos_tag(tokenized_words)\n",
        "\n",
        "print(tag)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('enjoy', 'VBP'), ('when', 'WRB'), ('I', 'PRP'), ('code', 'VBP'), ('machine', 'NN'), ('learning', 'NN'), ('algorithms', 'IN'), ('using', 'VBG'), ('python', 'JJ'), ('language', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN-x5qS6h6gd"
      },
      "source": [
        "**We install a file txt called 2001aspaceodyssey.txt and the goal is to apply the previous technics in this txt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLb8hlE5gnES"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "with open(\"2001aspaceodyssey.txt\",\"r\") as corpora:\n",
        "  sample_text = corpora.read()\n",
        "\n",
        "tokenized_sentences = sent_tokenize(sample_text)\n",
        "for sentence in tokenized_sentences:\n",
        "  tokenized_words = word_tokenize(sentence)\n",
        "  tag = pos_tag(tokenized_words)\n",
        "  #print(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfak7DxviVB3"
      },
      "source": [
        "# StopWords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poK3lYTAiahW"
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "with open(\"2001aspaceodyssey.txt\",\"r\") as corpora:\n",
        "  sample_text = corpora.read()\n",
        "\n",
        "tokenized_sentences = sent_tokenize(sample_text)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "#print(stop_words)\n",
        "\n",
        "tokenized_words = word_tokenize(sample_text)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for word in tokenized_words:\n",
        "  if word not in stop_words:\n",
        "    filtered_sentence.append(word)\n",
        "\n",
        "#print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnNeq_wJ24U_"
      },
      "source": [
        "# Named Entity Recognition : is the task of finding and classifying names in a text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUSLqjaP41ep"
      },
      "source": [
        "Example: \n",
        "\n",
        "**European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to after its practices**\n",
        "\n",
        "\n",
        "\n",
        "1.   European = Nationality/Region\n",
        "2.   Google = Organisation\n",
        "3.   $5.1 billion = Money\n",
        "4.   Wednesday = Date\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J8xTDKw23yC"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "\n",
        "sample_text = \"Trump economic advisor Larry Kudlow told Fox Bussiness Wednesday tha the administration supports\"\n",
        "\n",
        "tokenized_words = word_tokenize(sample_text)\n",
        "\n",
        "tag = pos_tag(tokenized_words)\n",
        "\n",
        "named_ent = nltk.ne_chunk(tag, binary = True)\n",
        "#named_ent.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibpHThKvYa9"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vccwO15euwYo",
        "outputId": "1101e95e-9cbe-45c4-b72f-096391af2ea9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "documents = []\n",
        "for category in movie_reviews.categories():\n",
        "  #print(category) # positive or negative\n",
        "  for fileid in movie_reviews.fileids(category):\n",
        "    #print(fileid)\n",
        "    review_word_list = list(movie_reviews.words(fileid))\n",
        "    #print(review_word_list)\n",
        "    document = (review_word_list, category)\n",
        "    documents.append(document)\n",
        "print(documents[0])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'], 'neg')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}